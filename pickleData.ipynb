{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from raw_data_processing import process_PAMAP2_all_data, process_hhar_all_files, process_motion_sense_all_files\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 (1218683, 3)\n",
      "109 (25368, 3)\n",
      "107 (937997, 3)\n",
      "106 (1082715, 3)\n",
      "104 (985200, 3)\n",
      "105 (1120516, 3)\n",
      "101 (1125961, 3)\n",
      "102 (1335439, 3)\n",
      "103 (757267, 3)\n",
      "108 (1218683, 3)\n",
      "109 (25368, 3)\n",
      "107 (937997, 3)\n",
      "106 (1082715, 3)\n",
      "104 (985200, 3)\n",
      "105 (1120516, 3)\n",
      "101 (1125961, 3)\n",
      "102 (1335439, 3)\n",
      "103 (757267, 3)\n",
      "108 (1218683, 3)\n",
      "109 (25368, 3)\n",
      "107 (937997, 3)\n",
      "106 (1082715, 3)\n",
      "104 (985200, 3)\n",
      "105 (1120516, 3)\n",
      "101 (1125961, 3)\n",
      "102 (1335439, 3)\n",
      "103 (757267, 3)\n",
      "108 (3656049, 3)\n",
      "109 (76104, 3)\n",
      "107 (2813991, 3)\n",
      "106 (3248145, 3)\n",
      "104 (2955600, 3)\n",
      "105 (3361548, 3)\n",
      "101 (3377883, 3)\n",
      "102 (4006317, 3)\n",
      "103 (2271801, 3)\n",
      "a (1665085, 3)\n",
      "b (1743413, 3)\n",
      "c (1636308, 3)\n",
      "d (1624117, 3)\n",
      "e (1812440, 3)\n",
      "f (1657155, 3)\n",
      "g (1396828, 3)\n",
      "h (1349133, 3)\n",
      "i (1415401, 3)\n",
      "a (1732665, 3)\n",
      "b (1843730, 3)\n",
      "c (1701482, 3)\n",
      "d (1713048, 3)\n",
      "e (1878796, 3)\n",
      "f (1562407, 3)\n",
      "g (1453907, 3)\n",
      "h (1423950, 3)\n",
      "i (1488022, 3)\n",
      "a (3397750, 3)\n",
      "b (3587143, 3)\n",
      "c (3337790, 3)\n",
      "d (3337165, 3)\n",
      "e (3691236, 3)\n",
      "f (3219562, 3)\n",
      "g (2850735, 3)\n",
      "h (2773083, 3)\n",
      "i (2903423, 3)\n",
      "test_run/original_datasets/motionsense/Data/B_Accelerometer_data\n",
      "test_run/original_datasets/motionsense/Data/B_Accelerometer_data/dws_1\n",
      "test_run/original_datasets/motionsense/Data/B_Accelerometer_data/dws_11\n",
      "test_run/original_datasets/motionsense/Data/B_Accelerometer_data/dws_2\n",
      "test_run/original_datasets/motionsense/Data/B_Accelerometer_data/jog_16\n",
      "test_run/original_datasets/motionsense/Data/B_Accelerometer_data/jog_9\n",
      "test_run/original_datasets/motionsense/Data/B_Accelerometer_data/sit_13\n",
      "test_run/original_datasets/motionsense/Data/B_Accelerometer_data/sit_5\n",
      "test_run/original_datasets/motionsense/Data/B_Accelerometer_data/std_14\n",
      "test_run/original_datasets/motionsense/Data/B_Accelerometer_data/std_6\n",
      "test_run/original_datasets/motionsense/Data/B_Accelerometer_data/ups_12\n",
      "test_run/original_datasets/motionsense/Data/B_Accelerometer_data/ups_3\n",
      "test_run/original_datasets/motionsense/Data/B_Accelerometer_data/ups_4\n",
      "test_run/original_datasets/motionsense/Data/B_Accelerometer_data/wlk_15\n",
      "test_run/original_datasets/motionsense/Data/B_Accelerometer_data/wlk_7\n",
      "test_run/original_datasets/motionsense/Data/B_Accelerometer_data/wlk_8\n",
      "test_run/original_datasets/motionsense/Data/C_Gyroscope_data\n",
      "test_run/original_datasets/motionsense/Data/C_Gyroscope_data/dws_1\n",
      "test_run/original_datasets/motionsense/Data/C_Gyroscope_data/dws_11\n",
      "test_run/original_datasets/motionsense/Data/C_Gyroscope_data/dws_2\n",
      "test_run/original_datasets/motionsense/Data/C_Gyroscope_data/jog_16\n",
      "test_run/original_datasets/motionsense/Data/C_Gyroscope_data/jog_9\n",
      "test_run/original_datasets/motionsense/Data/C_Gyroscope_data/sit_13\n",
      "test_run/original_datasets/motionsense/Data/C_Gyroscope_data/sit_5\n",
      "test_run/original_datasets/motionsense/Data/C_Gyroscope_data/std_14\n",
      "test_run/original_datasets/motionsense/Data/C_Gyroscope_data/std_6\n",
      "test_run/original_datasets/motionsense/Data/C_Gyroscope_data/ups_12\n",
      "test_run/original_datasets/motionsense/Data/C_Gyroscope_data/ups_3\n",
      "test_run/original_datasets/motionsense/Data/C_Gyroscope_data/ups_4\n",
      "test_run/original_datasets/motionsense/Data/C_Gyroscope_data/wlk_15\n",
      "test_run/original_datasets/motionsense/Data/C_Gyroscope_data/wlk_7\n",
      "test_run/original_datasets/motionsense/Data/C_Gyroscope_data/wlk_8\n"
     ]
    }
   ],
   "source": [
    "pamap_df = process_PAMAP2_all_data(\"test_run/original_datasets/PAMAP2\")\n",
    "hhar_df = process_hhar_all_files(\"test_run/original_datasets/hhar/Activity recognition exp\")\n",
    "motion_sense_df = process_motion_sense_all_files(\"test_run/original_datasets/motionsense/Data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the datasets\n",
    "with open('pickled_datasets/pamap2.pickle', 'wb') as file:\n",
    "    pickle.dump(pamap_df, file)\n",
    "with open('pickled_datasets/hhar2.pickle', 'wb') as file:\n",
    "    pickle.dump(hhar_df, file)\n",
    "with open('pickled_datasets/motionsense2.pickle', 'wb') as file:\n",
    "    pickle.dump(motion_sense_df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_datasets(datasets, sensor_type):\n",
    "    concated_datasets = {}\n",
    "    ##TODO need to check key to see whether mag exists\n",
    "    for df in datasets:\n",
    "        concated_datasets.update(df[sensor_type])\n",
    "    return concated_datasets\n",
    "\n",
    "cdf = concat_datasets([pamap_df, hhar_df, motion_sense_df], \"all\")\n",
    "def get_labels(data):\n",
    "    all_labels = []\n",
    "    for user in data:\n",
    "        all_labels = np.concatenate((np.unique(data[user][0][1]), all_labels))\n",
    "    labels = np.unique(all_labels)\n",
    "    return labels\n",
    "\n",
    "labels = get_labels(cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IMU ankle acc' 'IMU ankle gyro' 'IMU ankle mag' 'IMU chest acc'\n",
      " 'IMU chest gyro' 'IMU chest mag' 'IMU hand acc' 'IMU hand gyro'\n",
      " 'IMU hand mag' 'Phone acc' 'Phone gyro' 'Watch acc' 'Watch gyro'\n",
      " 'iphone Acc']\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhar_df_acc = hhar_df['acc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(hhar_df_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "016 (355418, 3)\n",
      "017 (366609, 3)\n",
      "029 (178716, 3)\n",
      "015 (418392, 3)\n",
      "014 (366487, 3)\n",
      "028 (165178, 3)\n",
      "010 (351649, 3)\n",
      "013 (369077, 3)\n",
      "012 (382414, 3)\n",
      "006 (408709, 3)\n",
      "023 (137646, 3)\n",
      "022 (337602, 3)\n",
      "008 (418989, 3)\n",
      "020 (371496, 3)\n",
      "021 (302247, 3)\n",
      "009 (154464, 3)\n",
      "025 (231729, 3)\n",
      "019 (297945, 3)\n",
      "018 (322271, 3)\n",
      "024 (170534, 3)\n",
      "026 (195172, 3)\n",
      "027 (158584, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def process_HARTH_back_acc_data(data_folder_path):\n",
    "    combined_data = []\n",
    "    for filename in os.listdir(data_folder_path):\n",
    "            file_path = os.path.join(data_folder_path, filename)\n",
    "            df = pd.read_csv(file_path, sep=',', header=0, na_filter=\"NaN\")\n",
    "            df['User-ID'] = filename[1:4]\n",
    "            combined_data.append(df)\n",
    "    df = pd.concat(combined_data)\n",
    "    df = df.replace(1, \"walking\")\n",
    "    df = df.replace(2, \"running\")\n",
    "    df = df.replace(3, \"shuffling\")\n",
    "    df = df.replace(4, \"stairs (ascending)\")\n",
    "    df = df.replace(5, \"stairs (descending)\")\n",
    "    df = df.replace(6, \"standing\")\n",
    "    df = df.replace(7, \"sitting\")\n",
    "    df = df.replace(8, \"lying\")\n",
    "    df = df.replace(13, \"cycling (sit)\")\n",
    "    df = df.replace(14, \"cycling (stand)\")\n",
    "    df = df.replace(130, \"cycling (sit, inactive)\")\n",
    "    df = df.replace(140, \"cycling (stand, inactive)\")\n",
    "    back_data = df.loc[:, ['User-ID', 'back_x', 'back_y', 'back_z', 'label']]\n",
    "    harth_user = back_data[\"User-ID\"].unique()\n",
    "\n",
    "\n",
    "    user_datasets = {}\n",
    "    for user in harth_user:\n",
    "        user_extract = back_data[back_data['User-ID'] == user]\n",
    "        data = user_extract[['back_x', 'back_y', 'back_z']].values\n",
    "        labels = user_extract[\"label\"].values\n",
    "        print(f\"{user} {data.shape}\")\n",
    "        user_datasets[user] = [(data, labels)]\n",
    "    return user_datasets\n",
    "\n",
    "combined_data = process_HARTH_back_acc_data(\"test_run/original_datasets/harth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['standing' 'standing' 'standing' ... 'sitting' 'sitting' 'sitting']\n"
     ]
    }
   ],
   "source": [
    "print(combined_data['006'][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import raw_data_processing\n",
    "raw_data_processing.store_pickle(combined_data, 'pickled_datasets/harth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "016 (355418, 3)\n",
      "017 (366609, 3)\n",
      "029 (178716, 3)\n",
      "015 (418392, 3)\n",
      "014 (366487, 3)\n",
      "028 (165178, 3)\n",
      "010 (351649, 3)\n",
      "013 (369077, 3)\n",
      "012 (382414, 3)\n",
      "006 (408709, 3)\n",
      "023 (137646, 3)\n",
      "022 (337602, 3)\n",
      "008 (418989, 3)\n",
      "020 (371496, 3)\n",
      "021 (302247, 3)\n",
      "009 (154464, 3)\n",
      "025 (231729, 3)\n",
      "019 (297945, 3)\n",
      "018 (322271, 3)\n",
      "024 (170534, 3)\n",
      "026 (195172, 3)\n",
      "027 (158584, 3)\n"
     ]
    }
   ],
   "source": [
    "def process_HARTH_thigh_acc_data(data_folder_path):\n",
    "    combined_data = []\n",
    "    for filename in os.listdir(data_folder_path):\n",
    "            file_path = os.path.join(data_folder_path, filename)\n",
    "            df = pd.read_csv(file_path, sep=',', header=0, na_filter=\"NaN\")\n",
    "            df['User-ID'] = filename[1:4]\n",
    "            combined_data.append(df)\n",
    "    df = pd.concat(combined_data)\n",
    "    df = df.replace(1, \"walking\")\n",
    "    df = df.replace(2, \"running\")\n",
    "    df = df.replace(3, \"shuffling\")\n",
    "    df = df.replace(4, \"stairs (ascending)\")\n",
    "    df = df.replace(5, \"stairs (descending)\")\n",
    "    df = df.replace(6, \"standing\")\n",
    "    df = df.replace(7, \"sitting\")\n",
    "    df = df.replace(8, \"lying\")\n",
    "    df = df.replace(13, \"cycling (sit)\")\n",
    "    df = df.replace(14, \"cycling (stand)\")\n",
    "    df = df.replace(130, \"cycling (sit, inactive)\")\n",
    "    df = df.replace(140, \"cycling (stand, inactive)\")\n",
    "    back_data = df.loc[:, ['User-ID', 'thigh_x', 'thigh_y', 'thigh_z', 'label']]\n",
    "    harth_user = back_data[\"User-ID\"].unique()\n",
    "\n",
    "\n",
    "    user_datasets = {}\n",
    "    for user in harth_user:\n",
    "        user_extract = back_data[back_data['User-ID'] == user]\n",
    "        data = user_extract[['thigh_x', 'thigh_y', 'thigh_z']].values\n",
    "        labels = user_extract[\"label\"].values\n",
    "        print(f\"{user} {data.shape}\")\n",
    "        user_datasets[user] = [(data, labels)]\n",
    "    return user_datasets\n",
    "\n",
    "combined_back_data = process_HARTH_thigh_acc_data(\"test_run/original_datasets/harth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_processing.store_pickle(hhar_df, 'pickled_datasets/hhar_HAR')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
